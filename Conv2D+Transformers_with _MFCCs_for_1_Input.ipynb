{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NXU0lgz_vBK"
   },
   "source": [
    "## CNN2D + Transformer (1 input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mt_h9B21AY5g",
    "outputId": "64bf3e6b-655d-4278-bd6d-136d54a149a5"
   },
   "source": [
    "In this notebook, we will try implement a CNN combined with a transformer model to make an ordinal classification. Only answer and reading tasks will be used as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E9d17uVBAbxS"
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import os, time, warnings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l7NS0ShiDj71"
   },
   "outputs": [],
   "source": [
    "def resize_spectrogram(spec, length, fact=-80):\n",
    "\n",
    "    # Create an empty canvas to put spectrogram into\n",
    "    canvas = np.ones((len(spec), length)) * fact\n",
    "\n",
    "    if spec.shape[1] <= length:\n",
    "        canvas[:, : spec.shape[1]] = spec\n",
    "    else:\n",
    "        canvas[:, :length] = spec[:, :length]\n",
    "    return canvas\n",
    "\n",
    "def compute_mel_spec(filename, sr=8000, hop_length=512, duration=30.0):\n",
    "\n",
    "    # Loads the mp3 file\n",
    "    y, sr = librosa.load(filename, sr=sr)\n",
    "\n",
    "    # Compute the mel spectrogram\n",
    "    x_mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "\n",
    "    # Apply logarithmic dB-scale to spectrogram and set maximum to 0 dB\n",
    "    x_mel = librosa.power_to_db(x_mel, ref=np.max)\n",
    "\n",
    "    # Compute mean strength per frequency for mel spectrogram\n",
    "    mel_strength = np.mean(x_mel, axis=1)\n",
    "\n",
    "    # Estimate the desired length of the spectrogram\n",
    "    length = int(duration * sr / hop_length)\n",
    "\n",
    "    # print(np.min(x_mel))\n",
    "    # print(np.max(x_mel))\n",
    "\n",
    "    # Put mel spectrogram into the right shape\n",
    "    x_mel = resize_spectrogram(x_mel, length)\n",
    "\n",
    "    x_mel = librosa.util.normalize(x_mel, axis=1)\n",
    "    x_mel = np.ones(x_mel.shape) + x_mel\n",
    "\n",
    "\n",
    "    return x_mel, mel_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "siJg1ZgLGXDl"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4Fe_e7jeDGgL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3Vz8DHqqYs_",
    "outputId": "a2bee7b3-0b1a-4269-ba49-50e375c0ab08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PAyxkEa8DKfY"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6nUrqctkSk3I"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lFeteD6XRaiH"
   },
   "outputs": [],
   "source": [
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, audio_file_list, scores):\n",
    "        self.audio_file_list = audio_file_list\n",
    "        self.scores = scores\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.audio_file_list[index]\n",
    "        # img, _ = librosa.load(img, sr=16000)\n",
    "        # print(type(img))\n",
    "        # img = torch.tensor(img, dtype=torch.float32)\n",
    "        # img = torch.mean(img, dim=0).unsqueeze(0)\n",
    "        # img = torchaudio.transforms.Spectrogram()(img)\n",
    "        img, _ = compute_mel_spec(f'/home/ongun/challenge/answer/{img}')\n",
    "        img = torch.tensor(img, dtype=torch.float32)\n",
    "        #print(img.shape)\n",
    "        # img = torch.mean(img, dim=0).unsqueeze(0)\n",
    "        # img = F.pad(img, (0, 1000 - img.shape[1]))\n",
    "        img = img.reshape([1, img.shape[0], img.shape[1]])\n",
    "        # img = generate_features(img)\n",
    "        # torch.index_select(img, 1, torch.LongTensor([2,0,1]))\n",
    "        #print(img.shape)\n",
    "        \n",
    "        def ordinal_labeler(score):\n",
    "            levels = [1]*score + [0]*(39 - score)\n",
    "            levels = torch.tensor(levels, dtype=torch.float32)\n",
    "            return levels\n",
    "        \n",
    "        score = self.scores[index]\n",
    "        label = np.array(ordinal_labeler(score))\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        count = len(self.audio_file_list)\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9ME-fbJQnQZz"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"modified_data.csv\")\n",
    "#vowel_df=df.iloc[::2]\n",
    "df=df.iloc[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GTQVESn0oQ_P",
    "outputId": "a1a2d428-29f1-4170-dd72-ccdd7ca00a0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['userid', 'filename', 'date', 'score', 'excercise_type'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "djy49TrcRktr"
   },
   "outputs": [],
   "source": [
    "audio_list = np.array(df['filename'].tolist())\n",
    "score_list = np.array(df['score'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "i0hHwBmfT7zt"
   },
   "outputs": [],
   "source": [
    "indices = np.arange(len(audio_list))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "limit = int(len(indices)*0.7)\n",
    "\n",
    "X_train, X_test = audio_list[indices[:limit]], audio_list[indices[limit:]]\n",
    "y_train, y_test = score_list[indices[:limit]], score_list[indices[limit:]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "l0YxzEPnWj9P"
   },
   "outputs": [],
   "source": [
    "train_data = MyCustomDataset(X_train, y_train)\n",
    "test_data = MyCustomDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kCMWes5zW1vd"
   },
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 40\n",
    "batch_size = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Q-Aq048QW-xe"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4eMmfg48XBXH"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BQmeMIP-BzPY"
   },
   "outputs": [],
   "source": [
    "def prediction2label(pred: np.ndarray, threshold: float):\n",
    "    \"\"\"Convert ordinal predictions to class labels, e.g.\n",
    "    \n",
    "    [0.9, 0.1, 0.1, 0.1] -> 0\n",
    "    [0.9, 0.9, 0.1, 0.1] -> 1\n",
    "    [0.9, 0.9, 0.9, 0.1] -> 2\n",
    "    etc.\n",
    "    \"\"\"\n",
    "    return (pred > threshold).cumprod(axis=1).sum(axis=1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7CyvAEpOXfuc"
   },
   "outputs": [],
   "source": [
    "importance_weights = torch.ones(39, dtype=torch.float).to(device)\n",
    "\n",
    "def loss_fn2(logits, levels, imp=importance_weights):\n",
    "    val = (-torch.sum((F.logsigmoid(logits)*levels\n",
    "                      + (F.logsigmoid(logits) - logits)*(1-levels))*imp,\n",
    "           dim=1))\n",
    "    return torch.mean(val)\n",
    "\n",
    "def loss_fn(logits, levels):\n",
    "    logits = prediction2label(logits)\n",
    "    modified_target = torch.zeros_like(logits)\n",
    "\n",
    "    # Fill in ordinal target function, i.e. 0 -> [1,0,0,...]\n",
    "    for i, target in enumerate(levels):\n",
    "        modified_target[i, 0:target+1] = 1\n",
    "\n",
    "    return nn.MSELoss(reduction='none')(logits, modified_target).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "CzqMGmfNXJd5"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.transformer_maxpool1 = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])\n",
    "        \n",
    "        # define single transformer encoder layer\n",
    "        # self-attention + feedforward network from \"Attention is All You Need\" paper\n",
    "        # 4 multi-head self-attention layers each with 40-->512--->40 feedforward network\n",
    "        transformer_layer1 = nn.TransformerEncoderLayer(\n",
    "            d_model=128, # input feature (frequency) dim after maxpooling 40*282 -> 40*70 (MFC*time)\n",
    "            nhead=4, # 4 self-attention layers in each multi-head self-attention layer in each encoder block\n",
    "            dim_feedforward=512, # 2 linear layers in each encoder block's feedforward network: dim 40-->512--->40\n",
    "            dropout=0.4, \n",
    "            activation='relu' # ReLU: avoid saturation/tame gradient/reduce compute time\n",
    "        )        \n",
    "        self.transformer_encoder1 = nn.TransformerEncoder(transformer_layer1, num_layers=4)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=7, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        \n",
    "        #self.fc1 = nn.Linear(111360+128, 512)\n",
    "        self.fc1= nn.Linear(55808,512)\n",
    "        #self.fc2 = nn.Linear(4096, 512)\n",
    "        self.fc3 = nn.Linear(512, 39)\n",
    "        # self.last = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.reshape([out.size(0), -1])\n",
    "        #print(out.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x_maxpool1 = self.transformer_maxpool1(x)\n",
    "\n",
    "        # remove channel dim: 1*40*70 --> 40*70\n",
    "        x_maxpool_reduced1 = torch.squeeze(x_maxpool1,1)\n",
    "        \n",
    "        # convert maxpooled feature map format: batch * freq * time ---> time * batch * freq format\n",
    "        # because transformer encoder layer requires tensor in format: time * batch * embedding (freq)\n",
    "        x_1 = x_maxpool_reduced1.permute(2,0,1) \n",
    "        \n",
    "        # finally, pass reduced input feature map x into transformer encoder layers\n",
    "        transformer_output_1 = self.transformer_encoder1(x_1)\n",
    "        \n",
    "        # create final feature emedding from transformer layer bytaking mean in the time dimension (now the 0th dim)\n",
    "        # transformer outputs 2x40 (MFCC embedding*time) feature map, take mean of columns i.e. take time average\n",
    "        transformer_embedding_1 = torch.mean(transformer_output_1, dim=0) # dim 40x70 --> 40\n",
    "        out = torch.cat([out,transformer_embedding_1], dim=1)  \n",
    "\n",
    "         \n",
    "        \n",
    "        \n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        #out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        # out = self.last(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQO12XbHjGND",
    "outputId": "00b2e28e-200f-486c-a25b-7c967ffcd830"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (transformer_maxpool1): MaxPool2d(kernel_size=[1, 4], stride=[1, 4], padding=0, dilation=1, ceil_mode=False)\n",
       "  (transformer_encoder1): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.4, inplace=False)\n",
       "        (dropout2): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.4, inplace=False)\n",
       "        (dropout2): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.4, inplace=False)\n",
       "        (dropout2): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.4, inplace=False)\n",
       "        (dropout2): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(7, 7), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (drop_out): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=55808, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=39, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "G6vksAstv8sw"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "id": "B1yBFUZpXONE",
    "outputId": "476f7f67-57c7-4385-f7c3-bdcc718f8e6e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0: acc: loss:  2106.3761077923264\n",
      "epoch: 1: acc: loss:  1347.7166709899902\n",
      "epoch: 2: acc: loss:  1344.7397148609161\n",
      "epoch: 3: acc: loss:  1390.698515176773\n",
      "epoch: 4: acc: loss:  1327.0460159778595\n",
      "epoch: 5: acc: loss:  1321.1217851638794\n",
      "epoch: 6: acc: loss:  1321.04714345932\n",
      "epoch: 7: acc: loss:  1287.9782330989838\n",
      "epoch: 8: acc: loss:  1373.1890199184418\n",
      "epoch: 9: acc: loss:  1202.7783708572388\n",
      "epoch: 10: acc: loss:  1196.7528923749924\n",
      "epoch: 11: acc: loss:  1008.4439240694046\n",
      "epoch: 12: acc: loss:  999.0041728019714\n",
      "epoch: 13: acc: loss:  887.0032672509551\n",
      "epoch: 14: acc: loss:  822.0994097329676\n",
      "epoch: 15: acc: loss:  643.3915438628756\n",
      "epoch: 16: acc: loss:  554.4169617407024\n",
      "epoch: 17: acc: loss:  539.7846945705824\n",
      "epoch: 18: acc: loss:  434.65060008158616\n",
      "epoch: 19: acc: loss:  407.3654048551907\n",
      "epoch: 20: acc: loss:  348.33986478898237\n",
      "epoch: 21: acc: loss:  188.86282402577467\n",
      "epoch: 22: acc: loss:  193.36830148733725\n",
      "epoch: 23: acc: loss:  194.7101021816443\n",
      "epoch: 24: acc: loss:  147.94830904959963\n",
      "epoch: 25: acc: loss:  217.2694587668475\n",
      "epoch: 26: acc: loss:  136.274273690165\n",
      "epoch: 27: acc: loss:  120.69145856149106\n",
      "epoch: 28: acc: loss:  0.004088493995368481\n",
      "epoch: 29: acc: loss:  0.0011497361119836569\n",
      "epoch: 30: acc: loss:  0.0007833517738617957\n",
      "epoch: 31: acc: loss:  106.25618285301789\n",
      "epoch: 32: acc: loss:  0.009479382075369358\n",
      "epoch: 33: acc: loss:  48.988457149690674\n",
      "epoch: 34: acc: loss:  0.0011733565479516983\n",
      "epoch: 35: acc: loss:  0.00696040503680706\n",
      "epoch: 36: acc: loss:  102.66641893577611\n",
      "epoch: 37: acc: loss:  0.00011098137474618852\n",
      "epoch: 38: acc: loss:  0.009419615380465984\n",
      "epoch: 39: acc: loss:  0.0003497460565995425\n",
      "epoch: 40: acc: loss:  7.617285882588476e-05\n",
      "epoch: 41: acc: loss:  198.6468484578099\n",
      "epoch: 42: acc: loss:  177.35060109837218\n",
      "epoch: 43: acc: loss:  72.45706154836569\n",
      "epoch: 44: acc: loss:  0.002407980151474476\n",
      "epoch: 45: acc: loss:  129.26014063944763\n",
      "epoch: 46: acc: loss:  93.85340550227247\n",
      "epoch: 47: acc: loss:  69.74092933987296\n",
      "epoch: 48: acc: loss:  92.59013594852661\n",
      "epoch: 49: acc: loss:  53.300319591977996\n",
      "epoch: 50: acc: loss:  0.0001337526919087395\n",
      "epoch: 51: acc: loss:  0.0013255819212645292\n",
      "epoch: 52: acc: loss:  0.0004715912218671292\n",
      "epoch: 53: acc: loss:  0.008798292838037014\n",
      "epoch: 54: acc: loss:  78.57541692484028\n",
      "epoch: 55: acc: loss:  58.60376800139804\n",
      "epoch: 56: acc: loss:  36.984755620140014\n",
      "epoch: 57: acc: loss:  0.0012227296829223633\n",
      "epoch: 58: acc: loss:  0.0007271766662597656\n",
      "epoch: 59: acc: loss:  8.344647994817933e-07\n",
      "epoch: 60: acc: loss:  19.625443268645896\n",
      "epoch: 61: acc: loss:  7.152555099310121e-07\n",
      "epoch: 62: acc: loss:  0.0\n",
      "epoch: 63: acc: loss:  9.5367431640625e-07\n",
      "epoch: 64: acc: loss:  0.0\n",
      "epoch: 65: acc: loss:  1.1920928244535389e-07\n",
      "epoch: 66: acc: loss:  0.0\n",
      "epoch: 67: acc: loss:  0.0\n",
      "epoch: 68: acc: loss:  1.1920928244535389e-07\n",
      "epoch: 69: acc: loss:  5.91278076171875e-05\n",
      "epoch: 70: acc: loss:  1.1920928244535389e-07\n",
      "epoch: 71: acc: loss:  1.9073486328125e-06\n",
      "epoch: 72: acc: loss:  7.867782187531702e-06\n",
      "epoch: 73: acc: loss:  2.765655517578125e-05\n",
      "epoch: 74: acc: loss:  2.8610202207346447e-06\n",
      "epoch: 75: acc: loss:  5.543231964111328e-05\n",
      "epoch: 76: acc: loss:  9.5367431640625e-07\n",
      "epoch: 77: acc: loss:  0.00019550323486328125\n",
      "epoch: 78: acc: loss:  2.86102294921875e-06\n",
      "epoch: 79: acc: loss:  0.0\n",
      "epoch: 80: acc: loss:  0.0\n",
      "epoch: 81: acc: loss:  9.5367431640625e-07\n",
      "epoch: 82: acc: loss:  9.775113539944869e-06\n",
      "epoch: 83: acc: loss:  0.0\n",
      "epoch: 84: acc: loss:  0.0\n",
      "epoch: 85: acc: loss:  5.960462772236497e-07\n",
      "epoch: 86: acc: loss:  1.1920928244535389e-07\n",
      "epoch: 87: acc: loss:  0.0\n",
      "epoch: 88: acc: loss:  0.0\n",
      "epoch: 89: acc: loss:  9.5367431640625e-07\n",
      "epoch: 90: acc: loss:  0.0\n",
      "epoch: 91: acc: loss:  1.1920928244535389e-07\n",
      "epoch: 92: acc: loss:  0.0\n",
      "epoch: 93: acc: loss:  0.0009541511535644531\n",
      "epoch: 94: acc: loss:  0.0\n",
      "epoch: 95: acc: loss:  1.0728836059570312e-06\n",
      "epoch: 96: acc: loss:  0.0\n",
      "epoch: 97: acc: loss:  9.5367431640625e-07\n",
      "epoch: 98: acc: loss:  0.00011444091796875\n",
      "epoch: 99: acc: loss:  0.0\n",
      "epoch: 100: acc: loss:  0.0\n",
      "epoch: 101: acc: loss:  0.00019669532775878906\n",
      "epoch: 102: acc: loss:  0.0\n",
      "epoch: 103: acc: loss:  9.5367431640625e-07\n",
      "epoch: 104: acc: loss:  0.0010023117065429688\n",
      "epoch: 105: acc: loss:  4.768367489305092e-06\n",
      "epoch: 106: acc: loss:  2.86102294921875e-06\n",
      "epoch: 107: acc: loss:  2.3841855067985307e-07\n",
      "epoch: 108: acc: loss:  9.5367431640625e-07\n",
      "epoch: 109: acc: loss:  2.0265558760002023e-06\n",
      "epoch: 110: acc: loss:  0.0\n",
      "epoch: 111: acc: loss:  1.1920928244535389e-07\n",
      "epoch: 112: acc: loss:  9.5367431640625e-05\n",
      "epoch: 113: acc: loss:  1.430511474609375e-05\n",
      "epoch: 114: acc: loss:  0.0\n",
      "epoch: 115: acc: loss:  5.841255187988281e-06\n",
      "epoch: 116: acc: loss:  0.0\n",
      "epoch: 117: acc: loss:  0.0\n",
      "epoch: 118: acc: loss:  0.0\n",
      "epoch: 119: acc: loss:  3.933898824470816e-06\n",
      "epoch: 120: acc: loss:  0.0024232864379882812\n",
      "epoch: 121: acc: loss:  1.1920928244535389e-07\n",
      "epoch: 122: acc: loss:  9.5367431640625e-07\n",
      "epoch: 123: acc: loss:  0.0\n",
      "epoch: 124: acc: loss:  3.576278118089249e-07\n",
      "epoch: 125: acc: loss:  0.0\n",
      "epoch: 126: acc: loss:  3.2186508178710938e-06\n",
      "epoch: 127: acc: loss:  2.8610211302293465e-06\n",
      "epoch: 128: acc: loss:  0.0\n",
      "epoch: 129: acc: loss:  1.1920928244535389e-07\n",
      "epoch: 130: acc: loss:  1.1920928244535389e-07\n",
      "epoch: 131: acc: loss:  2.3841856489070778e-07\n",
      "epoch: 132: acc: loss:  0.0\n",
      "epoch: 133: acc: loss:  7.62939453125e-06\n",
      "epoch: 134: acc: loss:  1.9073468138230965e-06\n",
      "epoch: 135: acc: loss:  0.0\n",
      "epoch: 136: acc: loss:  1.1920928244535389e-07\n",
      "epoch: 137: acc: loss:  2.86102294921875e-06\n",
      "epoch: 138: acc: loss:  9.5367431640625e-07\n",
      "epoch: 139: acc: loss:  2.0265558760002023e-06\n",
      "epoch: 140: acc: loss:  0.0\n",
      "epoch: 141: acc: loss:  0.0\n",
      "epoch: 142: acc: loss:  1.1920922133867862e-06\n",
      "epoch: 143: acc: loss:  9.059906005859375e-05\n",
      "epoch: 144: acc: loss:  4.768370445162873e-07\n",
      "epoch: 145: acc: loss:  1.52587890625e-05\n",
      "epoch: 146: acc: loss:  9.5367431640625e-07\n",
      "epoch: 147: acc: loss:  0.0007162094116210938\n",
      "epoch: 148: acc: loss:  0.0\n",
      "epoch: 149: acc: loss:  2.145764938177308e-06\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 150\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Run the forward pass\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "    \n",
    "        outputs = model(images)\n",
    "        loss = loss_fn2(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # correct = (predicted == labels).sum().item()\n",
    "        # acc_list.append(correct / total)\n",
    "        if np.sum(loss_list)<0.01:\n",
    "            break\n",
    "    print(f'epoch: {epoch}: acc:','loss: ',np.sum(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "5s2jLXcQR9iF"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"1inputtransformer-overfit.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(test_loader: DataLoader, model: nn.Module):\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    device=\"cuda\"\n",
    "    acc_list=[]\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "           \n",
    "            predictions = model(data)\n",
    "            \n",
    "            total = labels.size(0)\n",
    "            predicted = prediction2label(predictions, 0.5)\n",
    "            targets = prediction2label(labels, 0.5)\n",
    "            num_correct = (predicted == targets).sum().item()\n",
    "            acc_list.append(num_correct/total)\n",
    "        print(f\"Test Accuracy of the model: {np.mean(acc_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model: 0.0625\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(test_loader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJpaQryMUOS6",
    "outputId": "5e3f0597-f1a1-495c-962a-f9ebd34854f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([11], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([22], device='cuda:0')\n",
      "tensor([22], device='cuda:0')\n",
      "tensor([15], device='cuda:0')\n",
      "tensor([15], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([4], device='cuda:0')\n",
      "tensor([4], device='cuda:0')\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([5], device='cuda:0')\n",
      "tensor([5], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([12], device='cuda:0')\n",
      "tensor([12], device='cuda:0')\n",
      "tensor([22], device='cuda:0')\n",
      "tensor([22], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([12], device='cuda:0')\n",
      "tensor([12], device='cuda:0')\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([15], device='cuda:0')\n",
      "tensor([15], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([16], device='cuda:0')\n",
      "tensor([16], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([15], device='cuda:0')\n",
      "tensor([15], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([5], device='cuda:0')\n",
      "tensor([5], device='cuda:0')\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([19], device='cuda:0')\n",
      "tensor([19], device='cuda:0')\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([4], device='cuda:0')\n",
      "tensor([4], device='cuda:0')\n",
      "tensor([11], device='cuda:0')\n",
      "tensor([11], device='cuda:0')\n",
      "tensor([16], device='cuda:0')\n",
      "tensor([16], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([16], device='cuda:0')\n",
      "tensor([16], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([19], device='cuda:0')\n",
      "tensor([19], device='cuda:0')\n",
      "tensor([19], device='cuda:0')\n",
      "tensor([19], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([12], device='cuda:0')\n",
      "tensor([12], device='cuda:0')\n",
      "tensor([-1], device='cuda:0')\n",
      "tensor([-1], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([22], device='cuda:0')\n",
      "tensor([22], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([11], device='cuda:0')\n",
      "tensor([11], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([-1], device='cuda:0')\n",
      "tensor([-1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([5], device='cuda:0')\n",
      "tensor([5], device='cuda:0')\n",
      "tensor([16], device='cuda:0')\n",
      "tensor([16], device='cuda:0')\n",
      "tensor([4], device='cuda:0')\n",
      "tensor([4], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([23], device='cuda:0')\n",
      "tensor([23], device='cuda:0')\n",
      "tensor([25], device='cuda:0')\n",
      "tensor([25], device='cuda:0')\n",
      "tensor([21], device='cuda:0')\n",
      "tensor([21], device='cuda:0')\n",
      "tensor([11], device='cuda:0')\n",
      "tensor([11], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([16], device='cuda:0')\n",
      "tensor([16], device='cuda:0')\n",
      "tensor([22], device='cuda:0')\n",
      "tensor([22], device='cuda:0')\n",
      "tensor([4], device='cuda:0')\n",
      "tensor([4], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([30], device='cuda:0')\n",
      "tensor([30], device='cuda:0')\n",
      "tensor([5], device='cuda:0')\n",
      "tensor([5], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([11], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([27], device='cuda:0')\n",
      "tensor([27], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([17], device='cuda:0')\n",
      "tensor([17], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([4], device='cuda:0')\n",
      "tensor([4], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([12], device='cuda:0')\n",
      "tensor([10], device='cuda:0')\n",
      "tensor([5], device='cuda:0')\n",
      "tensor([5], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([16], device='cuda:0')\n",
      "tensor([16], device='cuda:0')\n",
      "tensor([11], device='cuda:0')\n",
      "tensor([11], device='cuda:0')\n",
      "tensor([12], device='cuda:0')\n",
      "tensor([12], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([5], device='cuda:0')\n",
      "tensor([5], device='cuda:0')\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([9], device='cuda:0')\n",
      "tensor([-1], device='cuda:0')\n",
      "tensor([-1], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n",
      "tensor([13], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "\n",
    "    outputs = model(images)\n",
    "    print(prediction2label(labels, 0.9))\n",
    "    print(prediction2label(outputs, 0.9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_h8KFrSq5CyY"
   },
   "source": [
    "Getting 0 loss and almost 0 accuracy implies that our model overfitted which was to be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
