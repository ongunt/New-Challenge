{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NXU0lgz_vBK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVUeSMn-Lu79"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mt_h9B21AY5g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E9d17uVBAbxS"
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import os, time, warnings\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l7NS0ShiDj71"
   },
   "outputs": [],
   "source": [
    "def resize_spectrogram(spec, length, fact=-80):\n",
    "\n",
    "    # Create an empty canvas to put spectrogram into\n",
    "    canvas = np.ones((len(spec), length)) * fact\n",
    "\n",
    "    if spec.shape[1] <= length:\n",
    "        canvas[:, : spec.shape[1]] = spec\n",
    "    else:\n",
    "        canvas[:, :length] = spec[:, :length]\n",
    "    return canvas\n",
    "\n",
    "def compute_mel_spec(filename, sr=8000, hop_length=512, duration=30.0):\n",
    "\n",
    "    # Loads the mp3 file\n",
    "    y, sr = librosa.load(filename, sr=sr)\n",
    "\n",
    "    # Compute the mel spectrogram\n",
    "    x_mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "\n",
    "    # Apply logarithmic dB-scale to spectrogram and set maximum to 0 dB\n",
    "    x_mel = librosa.power_to_db(x_mel, ref=np.max)\n",
    "\n",
    "    # Compute mean strength per frequency for mel spectrogram\n",
    "    mel_strength = np.mean(x_mel, axis=1)\n",
    "\n",
    "    # Estimate the desired length of the spectrogram\n",
    "    length = int(duration * sr / hop_length)\n",
    "\n",
    "    # print(np.min(x_mel))\n",
    "    # print(np.max(x_mel))\n",
    "\n",
    "    # Put mel spectrogram into the right shape\n",
    "    x_mel = resize_spectrogram(x_mel, length)\n",
    "\n",
    "    x_mel = librosa.util.normalize(x_mel, axis=1)\n",
    "    x_mel = np.ones(x_mel.shape) + x_mel\n",
    "\n",
    "\n",
    "    return x_mel, mel_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "siJg1ZgLGXDl"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4Fe_e7jeDGgL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3Vz8DHqqYs_",
    "outputId": "389adcb6-d782-4a1a-96df-0e79cb0c2022"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PAyxkEa8DKfY"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6nUrqctkSk3I"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lFeteD6XRaiH"
   },
   "outputs": [],
   "source": [
    "class MyVowelDataset(Dataset):\n",
    "    def __init__(self, audio_file_list, scores):\n",
    "        self.audio_file_list = audio_file_list\n",
    "        self.scores = scores\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.audio_file_list[index]\n",
    "        # img, _ = librosa.load(img, sr=16000)\n",
    "        # print(type(img))\n",
    "        # img = torch.tensor(img, dtype=torch.float32)\n",
    "        # img = torch.mean(img, dim=0).unsqueeze(0)\n",
    "        # img = torchaudio.transforms.Spectrogram()(img)\n",
    "        img, _ = compute_mel_spec(f'/home/ongun/challenge/answer/{img}')\n",
    "        img = torch.tensor(img, dtype=torch.float32)\n",
    "        #print(img.shape)\n",
    "        # img = torch.mean(img, dim=0).unsqueeze(0)\n",
    "        # img = F.pad(img, (0, 1000 - img.shape[1]))\n",
    "        img = img.reshape([1, img.shape[0], img.shape[1]])\n",
    "        # img = generate_features(img)\n",
    "        # torch.index_select(img, 1, torch.LongTensor([2,0,1]))\n",
    "        #print(img.shape)\n",
    "        \n",
    "        def ordinal_labeler(score):\n",
    "            levels = [1]*score + [0]*(39 - score)\n",
    "            levels = torch.tensor(levels, dtype=torch.float32)\n",
    "            return levels\n",
    "        \n",
    "        score = self.scores[index]\n",
    "        label = np.array(ordinal_labeler(score))\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        count = len(self.audio_file_list)\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QiN-htiSLqvG"
   },
   "outputs": [],
   "source": [
    "class MyAnswerDataset(Dataset):\n",
    "    def __init__(self, audio_file_list, scores):\n",
    "        self.audio_file_list = audio_file_list\n",
    "        self.scores = scores\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.audio_file_list[index]\n",
    "        # img, _ = librosa.load(img, sr=16000)\n",
    "        # print(type(img))\n",
    "        # img = torch.tensor(img, dtype=torch.float32)\n",
    "        # img = torch.mean(img, dim=0).unsqueeze(0)\n",
    "        # img = torchaudio.transforms.Spectrogram()(img)\n",
    "        img, _ = compute_mel_spec(f'/home/ongun/challenge/answer/{img}')\n",
    "        img = torch.tensor(img, dtype=torch.float32)\n",
    "        #print(img.shape)\n",
    "        # img = torch.mean(img, dim=0).unsqueeze(0)\n",
    "        # img = F.pad(img, (0, 1000 - img.shape[1]))\n",
    "        img = img.reshape([1, img.shape[0], img.shape[1]])\n",
    "        # img = generate_features(img)\n",
    "        # torch.index_select(img, 1, torch.LongTensor([2,0,1]))\n",
    "        #print(img.shape)\n",
    "        \n",
    "        def ordinal_labeler(score):\n",
    "            levels = [1]*score + [0]*(39 - score)\n",
    "            levels = torch.tensor(levels, dtype=torch.float32)\n",
    "            return levels\n",
    "        \n",
    "        score = self.scores[index]\n",
    "        label = np.array(ordinal_labeler(score))\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        count = len(self.audio_file_list)\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9ME-fbJQnQZz"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"modified_data.csv\")\n",
    "vowel_df=df.iloc[::2]\n",
    "answer_df=df.iloc[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GTQVESn0oQ_P",
    "outputId": "52acb29c-0534-4b8a-d135-09b79af97ade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['userid', 'filename', 'date', 'score', 'excercise_type'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IpvKlFeCLqvJ"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_classes = 40\n",
    "batch_size = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "djy49TrcRktr"
   },
   "outputs": [],
   "source": [
    "vowel_audio_list = np.array(vowel_df['filename'].tolist())\n",
    "vowel_score_list = np.array(vowel_df['score'].tolist())\n",
    "vowel_indices = np.arange(len(vowel_audio_list))\n",
    "np.random.shuffle(vowel_indices)\n",
    "\n",
    "vowel_limit = int(len(vowel_indices)*0.7)\n",
    "\n",
    "X_vowel_train, X_vowel_test = vowel_audio_list[vowel_indices[:vowel_limit]], vowel_audio_list[vowel_indices[vowel_limit:]]\n",
    "y_vowel_train, y_vowel_test = vowel_score_list[vowel_indices[:vowel_limit]], vowel_score_list[vowel_indices[vowel_limit:]]\n",
    "vowel_train_data = MyVowelDataset(X_vowel_train, y_vowel_train)\n",
    "vowel_test_data = MyVowelDataset(X_vowel_test, y_vowel_test)\n",
    "vowel_train_loader = DataLoader(dataset=vowel_train_data, batch_size=batch_size)\n",
    "vowel_test_loader = DataLoader(dataset=vowel_test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RjxZsj31LqvK"
   },
   "outputs": [],
   "source": [
    "answer_audio_list = np.array(answer_df['filename'].tolist())\n",
    "answer_score_list = np.array(answer_df['score'].tolist())\n",
    "answer_indices = np.arange(len(answer_audio_list))\n",
    "np.random.shuffle(answer_indices)\n",
    "\n",
    "answer_limit = int(len(answer_indices)*0.7)\n",
    "\n",
    "X_answer_train, X_answer_test = answer_audio_list[answer_indices[:answer_limit]], answer_audio_list[answer_indices[answer_limit:]]\n",
    "y_answer_train, y_answer_test = answer_score_list[answer_indices[:answer_limit]], answer_score_list[answer_indices[answer_limit:]]\n",
    "answer_train_data = MyAnswerDataset(X_answer_train, y_answer_train)\n",
    "answer_test_data = MyAnswerDataset(X_answer_test, y_answer_test)\n",
    "answer_train_loader = DataLoader(dataset=answer_train_data, batch_size=batch_size)\n",
    "answer_test_loader = DataLoader(dataset=answer_test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kCMWes5zW1vd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Q-Aq048QW-xe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "rOdMoC-WLqvK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BQmeMIP-BzPY"
   },
   "outputs": [],
   "source": [
    "def prediction2label(pred: np.ndarray, threshold: float):\n",
    "    \"\"\"Convert ordinal predictions to class labels, e.g.\n",
    "    \n",
    "    [0.9, 0.1, 0.1, 0.1] -> 0\n",
    "    [0.9, 0.9, 0.1, 0.1] -> 1\n",
    "    [0.9, 0.9, 0.9, 0.1] -> 2\n",
    "    etc.\n",
    "    \"\"\"\n",
    "    return (pred > threshold).cumprod(axis=1).sum(axis=1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7CyvAEpOXfuc"
   },
   "outputs": [],
   "source": [
    "importance_weights = torch.ones(39, dtype=torch.float).to(device)\n",
    "\n",
    "def loss_fn2(logits, levels, imp=importance_weights):\n",
    "    val = (-torch.sum((F.logsigmoid(logits)*levels\n",
    "                      + (F.logsigmoid(logits) - logits)*(1-levels))*imp,\n",
    "           dim=1))\n",
    "    return torch.mean(val)\n",
    "\n",
    "def loss_fn(logits, levels):\n",
    "    logits = prediction2label(logits)\n",
    "    modified_target = torch.zeros_like(logits)\n",
    "\n",
    "    # Fill in ordinal target function, i.e. 0 -> [1,0,0,...]\n",
    "    for i, target in enumerate(levels):\n",
    "        modified_target[i, 0:target+1] = 1\n",
    "\n",
    "    return nn.MSELoss(reduction='none')(logits, modified_target).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CzqMGmfNXJd5"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "########### First Transformer Layer\n",
    "        \n",
    "        self.transformer_maxpool1 = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])\n",
    "        \n",
    "        # define single transformer encoder layer\n",
    "        # self-attention + feedforward network from \"Attention is All You Need\" paper\n",
    "        # 4 multi-head self-attention layers each with 40-->512--->40 feedforward network\n",
    "        transformer_layer1 = nn.TransformerEncoderLayer(\n",
    "            d_model=128, # input feature (frequency) dim after maxpooling 40*282 -> 40*70 (MFC*time)\n",
    "            nhead=4, # 4 self-attention layers in each multi-head self-attention layer in each encoder block\n",
    "            dim_feedforward=512, # 2 linear layers in each encoder block's feedforward network: dim 40-->512--->40\n",
    "            dropout=0.4, \n",
    "            activation='relu' # ReLU: avoid saturation/tame gradient/reduce compute time\n",
    "        )        \n",
    "        self.transformer_encoder1 = nn.TransformerEncoder(transformer_layer1, num_layers=4)\n",
    "\n",
    "############ Second Transformer Layer for the second input\n",
    "        self.transformer_maxpool2 = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])\n",
    "        \n",
    "        # define single transformer encoder layer\n",
    "        # self-attention + feedforward network from \"Attention is All You Need\" paper\n",
    "        # 4 multi-head self-attention layers each with 40-->512--->40 feedforward network\n",
    "        transformer_layer2 = nn.TransformerEncoderLayer(\n",
    "            d_model=128, # input feature (frequency) dim after maxpooling 40*282 -> 40*70 (MFC*time)\n",
    "            nhead=4, # 4 self-attention layers in each multi-head self-attention layer in each encoder block\n",
    "            dim_feedforward=512, # 2 linear layers in each encoder block's feedforward network: dim 40-->512--->40\n",
    "            dropout=0.4, \n",
    "            activation='relu' # ReLU: avoid saturation/tame gradient/reduce compute time\n",
    "        )        \n",
    "        self.transformer_encoder2 = nn.TransformerEncoder(transformer_layer2, num_layers=4)\n",
    "        \n",
    "######## First Conv        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv1_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=7, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4))\n",
    "        self.conv1_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4))\n",
    "        self.conv1_layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "######### Second Conv Block        \n",
    "        self.conv2_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=7, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4))\n",
    "        self.conv2_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4))\n",
    "        self.conv2_layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))        \n",
    "######## Fully Connected        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.drop_out = nn.Dropout()\n",
    "        \n",
    "        #self.fc1 = nn.Linear((111360+128)*2, 4096)\n",
    "        #self.fc1 = nn.Linear(111616, 512)\n",
    "        #self.fc2 = nn.Linear(4096, 512)\n",
    "        self.fc1 = nn.Linear(5632, 39)\n",
    "        #self.fc3 = nn.Linear(512, 39)\n",
    "        # self.last = nn.Softmax(dim=1)\n",
    "    def forward(self, x1,x2):\n",
    "        out_1 = self.conv1_layer1(x1)\n",
    "        out_1 = self.conv1_layer2(out_1)\n",
    "        out_1 = self.conv1_layer3(out_1)\n",
    "        out_1 = out_1.reshape([out_1.size(0), -1])\n",
    "        # print(out.shape)\n",
    "        out_2= self.conv2_layer1(x2)\n",
    "        out_2= self.conv2_layer2(out_2)\n",
    "        out_2= self.conv2_layer3(out_2)\n",
    "        out_2= out_2.reshape([out_2.size(0), -1])\n",
    "######### Transformer output        \n",
    "        \n",
    "        x_maxpool1 = self.transformer_maxpool1(x1)\n",
    "\n",
    "        # remove channel dim: 1*40*70 --> 40*70\n",
    "        x_maxpool_reduced1 = torch.squeeze(x_maxpool1,1)\n",
    "        \n",
    "        # convert maxpooled feature map format: batch * freq * time ---> time * batch * freq format\n",
    "        # because transformer encoder layer requires tensor in format: time * batch * embedding (freq)\n",
    "        x_1 = x_maxpool_reduced1.permute(2,0,1) \n",
    "        \n",
    "        # finally, pass reduced input feature map x into transformer encoder layers\n",
    "        transformer_output_1 = self.transformer_encoder1(x_1)\n",
    "        \n",
    "        # create final feature emedding from transformer layer bytaking mean in the time dimension (now the 0th dim)\n",
    "        # transformer outputs 2x40 (MFCC embedding*time) feature map, take mean of columns i.e. take time average\n",
    "        transformer_embedding_1 = torch.mean(transformer_output_1, dim=0) # dim 40x70 --> 40\n",
    "        \n",
    "########### Transformer output\n",
    "        x_maxpool2 = self.transformer_maxpool2(x2)\n",
    "\n",
    "        # remove channel dim: 1*40*70 --> 40*70\n",
    "        x_maxpool_reduced2 = torch.squeeze(x_maxpool2,1)\n",
    "        \n",
    "        # convert maxpooled feature map format: batch * freq * time ---> time * batch * freq format\n",
    "        # because transformer encoder layer requires tensor in format: time * batch * embedding (freq)\n",
    "        x_2 = x_maxpool_reduced2.permute(2,0,1) \n",
    "        \n",
    "        # finally, pass reduced input feature map x into transformer encoder layers\n",
    "        transformer_output_2 = self.transformer_encoder2(x_2)\n",
    "        \n",
    "        # create final feature emedding from transformer layer bytaking mean in the time dimension (now the 0th dim)\n",
    "        # transformer outputs 2x40 (MFCC embedding*time) feature map, take mean of columns i.e. take time average\n",
    "        transformer_embedding_2 = torch.mean(transformer_output_2, dim=0) # dim 40x70 --> 40\n",
    "        \n",
    "        \n",
    "        out = torch.cat([out_1,transformer_embedding_1,out_2,transformer_embedding_2], dim=1)  \n",
    "         \n",
    "        \n",
    "        \n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        #out = self.fc2(out)\n",
    "        #out = self.fc3(out)\n",
    "        # out = self.last(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQO12XbHjGND",
    "outputId": "8a7b574e-893b-4e3c-88c8-e550ccb948ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (transformer_maxpool1): MaxPool2d(kernel_size=[1, 4], stride=[1, 4], padding=0, dilation=1, ceil_mode=False)\n",
       "  (transformer_encoder1): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.4, inplace=False)\n",
       "        (dropout2): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.4, inplace=False)\n",
       "        (dropout2): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.4, inplace=False)\n",
       "        (dropout2): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.4, inplace=False)\n",
       "        (dropout2): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer_maxpool2): MaxPool2d(kernel_size=[1, 4], stride=[1, 4], padding=0, dilation=1, ceil_mode=False)\n",
       "  (transformer_encoder2): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.4, inplace=False)\n",
       "        (dropout2): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.4, inplace=False)\n",
       "        (dropout2): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.4, inplace=False)\n",
       "        (dropout2): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.4, inplace=False)\n",
       "        (dropout2): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv1_layer1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(7, 7), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv1_layer2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv1_layer3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2_layer1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(7, 7), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2_layer2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2_layer3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (drop_out): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=5632, out_features=39, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "G6vksAstv8sw"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "B1yBFUZpXONE",
    "outputId": "0774bca8-1c0f-4b9e-cfb6-03916cc2dc70"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d6320f5130b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvowel_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(len(vowel_train_loader))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vowel_train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "total_step = len(vowel_train_loader)\n",
    "#print(len(vowel_train_loader))\n",
    "num_epochs=150\n",
    "for epoch in range(num_epochs):\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    for  (images1, labels),(images2,labels2) in zip(vowel_train_loader,answer_train_loader):\n",
    "        # Run the forward pass\n",
    "        images1 = images1.cuda()\n",
    "        images2 = images2.cuda()\n",
    "        labels = labels.cuda()\n",
    "    \n",
    "        outputs = model(images1,images2)\n",
    "        loss = loss_fn2(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # correct = (predicted == labels).sum().item()\n",
    "        # acc_list.append(correct / total)\n",
    "\n",
    "    print(f'epoch: {epoch}: acc:','loss: ',np.sum(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5s2jLXcQR9iF"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_2inputmodel1-150epoch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJpaQryMUOS6",
    "outputId": "7c4d7736-3167-4139-c9c9-12097f9dea6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for (images1, labels1),(images2, labels2) in zip(vowel_train_loader,answer_train_loader):\n",
    "  images1 = images1.cuda()\n",
    "  labels1 = labels1.cuda()\n",
    "  images2 = images2.cuda()\n",
    "  outputs = model(images1,images2)\n",
    "  #print(prediction2label(labels, 0.9))\n",
    "  #print(prediction2label(outputs, 0.9))\n",
    "  print(images1)\n",
    "  print(images2)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_h8KFrSq5CyY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
